# STEP 43 신경망

## 43.1 DeZero의 linear 함수
이전 단계에서의 선형 회귀로 수행한 계산은 '행렬의 곱'과 '행렬의 덧셈'이다.  
y = F.matmul(x, W) + b
이렇게 입력 x와 매개변수 W사이에서 행렬 곱을 구하고, b를 더하는 것을 선형 변환 혹은 아핀 변환 이라고 한다. 

선형변환을 linear함수로 구현해보자. 구현방법은 두 가지이다.

<image src = "../../밑바닥3 그림과 수식/그림 43-1.png">
1. 지금까지 구현해온 DeZero 함수 사용
2. Function 클래스 상속하여 새롭게 Linear 함수 클래스 구현
위 그림과 같이 후자가 더 메모리를 효율적으로 쓴다.    

그러나, 전자처럼 DeZero함수를 사용하면서 메모리 효율도 개선할 수 있는 방법이 있다.

```python
def linear_simple(x, W, b=None):
    t = matmul(x, W)
    if b is None:
        return t

    y = t + b
    t.data = None  # t의 데이터 삭제
    return y
```
x, W는 Variable 혹은 ndarray의 인스턴스라고 가정한다.  
ndarray라면 matmul함수 안에서 Variable 인스턴스로 변환된다. 
중간 결과인 t는 역전파 시 아무에게도 필요치 않으므로 (ㅜㅜ) 메모리 효율을 위해 삭제한다. 
저렇게 하면 참조카운트가 0이 되어서 파이썬 인터프리터에 의해 삭제된다.   
신경망에서 메모리의 대부분을 차지하는 것이 중간 결과 텐서이므로 불필요한 텐서는 즉시 삭제해야 한다.  자동화하는 방법도 존재함(Aggressive Buffer Release)

## 43.2 비선형 데이터셋
더 복잡한 데이터셋에 도전해보자.
```python
import numpy as np

np.random.seed(0)
x = np.random.rand(100, 1)
y = np.sin(2 * np.pi * x) + np.random.rand(100, 1) # 데이터 생성에 sin함수 사용
```
<image src = "../../밑바닥3 그림과 수식/그림 43-2.png"> 
그래프를 보면 x와 y는 선형관계가 아니다. 이러한 비선형 데이터셋은 선형회귀로 풀 수 없다.   
-> 신경망이 해결사로 등장하는 순간!


## 43.3 활성화 함수와 신경망
선형 변환은 말 그대로 입력 데이터를 선형으로 변환해준다.  
그런데 신경망은 선형 변환의 출력에 더해 비선형 변환을 수행하는데, 이때 비선형 변환을 활성화 함수라고 한다.   
대표적으로는 ReLU함수와 sigmoid 함수가 있다. (다른것도 엄청 많음!)
여기서는 활성화함수로 시그모이드 함수를 사용한다.
<image src = "../../밑바닥3 그림과 수식/식 43.1.png">     
<image src = "../../밑바닥3 그림과 수식/그림 43-3.png"> 
```python
def sigmoid_simple(x):
    x = as_variable(x)
    y = 1 / (1 + exp(-x))
    return y
```

## 43.4 신경망 구현
일반적인 신경망은 '선형 변환 -> 활성화 함수 -> 선형 변환 -> 활성화 함수 -> ...' 형태로 연속적으로 변환을 수행한다.   
2층 신경망 예시는 다음과 같다.  
```python
W1, b1 = Varibale(...), Variable(...)
W2, b2 = Varibale(...), Variable(...)

def predict(x):
    y = F.linear(x, W1, b1)
    y = F.sigmoid(y)
    y = F.linear(y, W2, b2)
    return y

```  
위 코드가 신경망 추론(predict)코드이다.   
신경망 학습에서는 추론을 처리한 후, 손실 함수를 추가하고, 손실 함수의 출력을 최소화하는 매개변수를 찾는다.  
이것이 신경망의 학습이다.

<image src = "../../밑바닥3 그림과 수식/그림 43-4.png"> 

sin함수의 곡선을 잘 표현하고 있는 것을 볼 수 있다.   
-> 선형회귀의 구현에 활성화 함수와 선형 변환을 거듭 적용하여 비선형 관계를 학습했다.   
-> 더 깊은 신경망도 구현할 수 있다. 하지만 층이 깊어지질수록 매개변수 관리가 어려워진다.  
